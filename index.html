<!DOCTYPE html>
<html lang="es">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Juan Manuel Hernandez Rivera</title>
    <meta name="description" content="Creative CV is a HTML resume template for professionals. Built with Bootstrap 4, Now UI Kit and FontAwesome, this modern and responsive design template is perfect to showcase your portfolio, skils and experience."/>
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700,200" rel="stylesheet">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css" rel="stylesheet">
    <link href="css/aos.css" rel="stylesheet">
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="styles/main.css" rel="stylesheet">
  </head>
  <body id="top">
    <header>
      <div class="profile-page sidebar-collapse">
        <nav class="navbar navbar-expand-lg fixed-top navbar-transparent bg-primary" color-on-scroll="400">
          <div class="container">
            <div class="navbar-translate"><a class="navbar-brand" href="#" rel="tooltip">HOME</a>
              <button class="navbar-toggler navbar-toggler" type="button" data-toggle="collapse" data-target="#navigation" aria-controls="navigation" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-bar bar1"></span><span class="navbar-toggler-bar bar2"></span><span class="navbar-toggler-bar bar3"></span></button>
            </div>
            <div class="collapse navbar-collapse justify-content-end" id="navigation">
              <ul class="navbar-nav">
                <li class="nav-item"><a class="nav-link smooth-scroll" href="#about">About Me</a></li>
                <li class="nav-item"><a class="nav-link smooth-scroll" href="#skill">Skills</a></li>
                <li class="nav-item"><a class="nav-link smooth-scroll" href="#portfolio">Portfolio</a></li>
                <li class="nav-item"><a class="nav-link smooth-scroll" href="#experience">Experience</a></li>
                <li class="nav-item"><a class="nav-link smooth-scroll" href="#contact">Contact</a></li>
              </ul>
            </div>
          </div>
        </nav>
      </div>
    </header>
    <div class="page-content">
      <div>
<div class="profile-page">
  <div class="wrapper">
    <div class="page-header page-header-small" filter-color="blue">
      <div class="page-header-image" data-parallax="true" style="background-image: url('images/cabecera-1.webp');"></div>
      <div class="container">
        <div class="content-center">
          <div class="cc-profile-image"><a href="#"><img src="images/profile-image.webp" alt="Image"/></a></div>
          <div class="h2 title">Manuel Hernandez</div>
          <p class="category text-white">Data Specialist</p>
          <a class="btn btn-primary smooth-scroll mr-2" href="#contact" data-aos="zoom-in" data-aos-anchor="data-aos-anchor">Schedule Meting</a>
          <a class="btn btn-primary" href="Curriculum.pdf" target="_blank" data-aos="zoom-in" data-aos-anchor="data-aos-anchor">Download CV</a>
        </div>
      </div>
      <div class="section">
        <div class="container">
          <div class="button-container">
            <!-- <a class="btn btn-default btn-round btn-lg btn-icon" href="https://join.skype.com/invite/vfcMypZd0Ucf" target="_blank" rel="tooltip" title="Contáctame en Skype"><i class="fa fa-skype"></i></a> -->
            <a class="btn btn-default btn-round btn-lg btn-icon smooth-scroll" href="#contact"  target="_blank" rel="tooltip" title="Contact Me"><i class="fa fa-phone"></i></a>
            <a class="btn btn-default btn-round btn-lg btn-icon" href="https://www.linkedin.com/in/jmhernandezr/" target="_blank" rel="tooltip" title="Follow me on LinkedIn"><i class="fa fa-linkedin"></i></a>
            <a class="btn btn-default btn-round btn-lg btn-icon" href="https://github.com/jmhrivera" target="_blank" rel="tooltip" title="Check out my repositories"><i class="fa fa-github"></i></a>
            <!-- <a class="btn btn-default btn-round btn-lg btn-icon" href="https://es.scribd.com/user/330352375/Jesus-Benjamin-Zerpa" target="_blank" rel="tooltip" title="Comparto mis libros"><i class="fa fa-scribd"></i></a></div> -->
        </div>
      </div>
    </div>
  </div>
</div>
<div class="section" id="about">
  <div class="section" id="about">
    <div class="container">
      <div class="card" data-aos="fade-up" data-aos-offset="10">
        <div class="row">
        </div>
        <div class="col-md-12">
          <div class="card-body">
            <div class="h4 mt-0 title">General Info</div>
            <div class="row">
              <div class="col-sm-4"><strong class="text-uppercase">Age:</strong></div>
              <div class="col-sm-8">33</div>
            </div>
            <div class="row mt-3">
              <div class="col-sm-4"><strong class="text-uppercase">Email:</strong></div>
              <div class="col-sm-8">jmhernandezr2@gmail.com</div>
            </div>
            <div class="row mt-3">
              <div class="col-sm-4"><strong class="text-uppercase">Phone Number:</strong></div>
              <div class="col-sm-8">+521-55-8615-0878</div>
            </div>
            <div class="row mt-3">
              <div class="col-sm-4"><strong class="text-uppercase">Dirección:</strong></div>
              <div class="col-sm-8">Merida, Mexico.</div>
            </div>
            <div class="row mt-3">
              <div class="col-sm-4"><strong class="text-uppercase">Languages:</strong></div>
              <div class="col-sm-8">Spanish, English, Portuguese.</div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="container">
    <div class="card" data-aos="fade-up" data-aos-offset="10">
      <div class="row">
        <div class=" col-md-12">
          <div class="card-body">
            <div class="h4 mt-0 title">About Me</div>
            <p>Hi! Welcome to my site, I'm JuanMa.</p>
            <p>
            I am a passionate Data Scientist with nine years of experience in <strong>data analysis</strong>, <strong>reporting</strong>, and <strong>presentations</strong>. I excel in uncovering insights that enhance business performance using tools such as <strong>Python</strong>, <strong>SQL</strong>, and <strong>Power BI</strong>. My journey began at <strong>Grupo Expansión</strong>, where I applied advanced statistical techniques to data from high advertising investment sectors, developing metrics and reports that supported the sales team in providing solutions to clients.
            </p>
            <p>
            Later, I joined <strong>La Jornada</strong> as a <strong>Business Intelligence Manager</strong>, leading a strategic initiative to align business objectives with operational activities. Managing a team of six, I utilized tools like <strong>Ipsos</strong> and <strong>ComScore</strong> and developed comprehensive <strong>Power BI dashboards</strong>, enabling data-driven decision-making. This role refined my ability to translate technical data into actionable strategies that improved market understanding and ad performance.
            </p>
            <p>
            At <strong>The Nielsen Company</strong>, I served as a <strong>Technical Account Manager</strong>, providing training and support for <strong>digital solutions</strong> focused on campaign performance analysis. I managed client relationships post-sale, increasing retention and engagement by making technical insights accessible and actionable for business stakeholders.
            </p>
            <p>
            I further strengthened my academic background at the <strong>National Technological Institute of Mexico</strong>, specializing in <strong>Data Science</strong> and <strong>Big Data</strong>. I mastered data manipulation and visualization tools such as <strong>Pandas</strong>, <strong>NumPy</strong>, and <strong>Matplotlib</strong>, while deepening my understanding of Python programming. My intensive learning at the <strong>Tripleten Bootcamp</strong> allowed me to excel in machine learning techniques using <strong>Scikit-Learn</strong> and <strong>PyTorch</strong>, leading projects that included <strong>predicting customer churn</strong> and <strong>age verification</strong> through computer vision.
            </p>
            <p>
            Today, as a committed Data Scientist, I offer high-quality services for strategic decision-making in resource management. Focused on innovation, I strive to help companies move beyond traditional analysis methods, offering a comprehensive perspective that enhances control and understanding of <strong>ROI behavior</strong>. For more about my career and projects, please continue reading my web content and visit my project portfolio.
            </p>
             <!-- <p ALIGN="justify"></p> -->
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<!-- TECHNICAL SKILLS -->
<div class="section" id="skill">
  <div class="container">
    <div class="h4 text-center mb-4 title">Technical Skills</div>
    <div class="card" data-aos="fade-up" data-aos-anchor-placement="top-bottom">
      <div class="card-body">
        <div class="row">
          <div class="col-md-6">
            <div class="progress-container progress-primary"><span class="progress-badge">Python</span>
              <div class="progress">
                <div class="progress-bar progress-bar-primary" data-aos="progress-full" data-aos-offset="10" data-aos-duration="2000" role="progressbar" aria-valuenow="60" aria-valuemin="0" aria-valuemax="100" style="width: 100%;"></div>
              </div>
            </div>
          </div>
          <div class="col-md-6">
            <div class="progress-container progress-primary"><span class="progress-badge">Numpy</span>
              <div class="progress">
                <div class="progress-bar progress-bar-primary" data-aos="progress-full" data-aos-offset="10" data-aos-duration="2000" role="progressbar" aria-valuenow="60" aria-valuemin="0" aria-valuemax="100" style="width: 100%;"></div>
              </div>
            </div>
          </div>
        </div>
        <div class="row">
          <div class="col-md-6">
            <div class="progress-container progress-primary"><span class="progress-badge">Pandas</span>
              <div class="progress">
                <div class="progress-bar progress-bar-primary" data-aos="progress-full" data-aos-offset="10" data-aos-duration="2000" role="progressbar" aria-valuenow="60" aria-valuemin="0" aria-valuemax="100" style="width: 100%;"></div>
              </div>
            </div>
          </div>
          <div class="col-md-6">
            <div class="progress-container progress-primary"><span class="progress-badge">Matplotlib</span>
              <div class="progress">
                <div class="progress-bar progress-bar-primary" data-aos="progress-full" data-aos-offset="10" data-aos-duration="2000" role="progressbar" aria-valuenow="60" aria-valuemin="0" aria-valuemax="100" style="width: 100%;"></div>
              </div>
            </div>
          </div>
        </div>
        <div class="row">
          <div class="col-md-6">
            <div class="progress-container progress-primary"><span class="progress-badge">Scikit-Learn</span>
              <div class="progress">
                <div class="progress-bar progress-bar-primary" data-aos="progress-full" data-aos-offset="10" data-aos-duration="2000" role="progressbar" aria-valuenow="60" aria-valuemin="0" aria-valuemax="100" style="width: 100%;"></div>
              </div>
            </div>
          </div>
          <div class="col-md-6">
            <div class="progress-container progress-primary"><span class="progress-badge">GitHub</span>
              <div class="progress">
                <div class="progress-bar progress-bar-primary" data-aos="progress-full" data-aos-offset="10" data-aos-duration="2000" role="progressbar" aria-valuenow="60" aria-valuemin="0" aria-valuemax="100" style="width: 100%;"></div>
              </div>
            </div>
          </div>
          <div class="col-md-6">
            <div class="progress-container progress-primary"><span class="progress-badge">SQL</span>
              <div class="progress">
                <div class="progress-bar progress-bar-primary" data-aos="progress-full" data-aos-offset="10" data-aos-duration="2000" role="progressbar" aria-valuenow="60" aria-valuemin="0" aria-valuemax="100" style="width: 100%;"></div>
              </div>
            </div>
          </div>
          <div class="col-md-6">
            <div class="progress-container progress-primary"><span class="progress-badge">Looker Studio</span>
              <div class="progress">
                <div class="progress-bar progress-bar-primary" data-aos="progress-full" data-aos-offset="10" data-aos-duration="2000" role="progressbar" aria-valuenow="60" aria-valuemin="0" aria-valuemax="100" style="width: 100%;"></div>
              </div>
            </div>
          </div>
          <div class="col-md-6">
            <div class="progress-container progress-primary"><span class="progress-badge">Pytorch</span>
              <div class="progress">
                <div class="progress-bar progress-bar-primary" data-aos="progress-full" data-aos-offset="10" data-aos-duration="2000" role="progressbar" aria-valuenow="60" aria-valuemin="0" aria-valuemax="100" style="width: 100%;"></div>
              </div>
            </div>
          </div>
          <div class="col-md-6">
            <div class="progress-container progress-primary"><span class="progress-badge">Keras</span>
              <div class="progress">
                <div class="progress-bar progress-bar-primary" data-aos="progress-full" data-aos-offset="10" data-aos-duration="2000" role="progressbar" aria-valuenow="60" aria-valuemin="0" aria-valuemax="100" style="width: 100%;"></div>      
              </div>
            </div>
          </div>
          




        </div>
      </div>
    </div>
  </div>
</div>


<!-- PORFOLIO -->
<div class="section" id="portfolio">
  <div class="container">
    <div class="row">
      <div class="col-md-6 ml-auto mr-auto">
        <div class="h4 text-center mb-4 title">Portfolio</div>
         <div class="nav-align-center">
        </div>
      </div>
    </div>

<!-- PROJECTS -->
<div class="container">
  <div class="portfolio-grid">
      <!-- Reemplaza las imágenes y títulos con tus propios proyectos -->
      <div class="portfolio-item" onclick="toggleDescription(0)">P1 Data Cleaning</div>
      <div class="portfolio-item" onclick="toggleDescription(1)">P2 Data Preprocessing</div>
      <div class="portfolio-item" onclick="toggleDescription(2)">P3 Extract Data Analysis</div>
      <div class="portfolio-item" onclick="toggleDescription(3)">P4 Hypothesis Testing</div>
      <div class="portfolio-item" onclick="toggleDescription(4)">P5 Streamlit Dashboard</div>
      <div class="portfolio-item" onclick="toggleDescription(5)">P6 API and Web Scrapping</div>
      <div class="portfolio-item" onclick="toggleDescription(6)">P7 Complete Data Analysis</div>
      <div class="portfolio-item" onclick="toggleDescription(7)">P8 ML Classification</div>
      <div class="portfolio-item" onclick="toggleDescription(8)">P9 ML Churn Prediction</div>
      <div class="portfolio-item" onclick="toggleDescription(9)">P10 ML Oil Prediction</div>
      <div class="portfolio-item" onclick="toggleDescription(10)">P11 ML Insurance Prediction</div>
      <div class="portfolio-item" onclick="toggleDescription(11)">P12 ML Car Price Prediction</div>
      <div class="portfolio-item" onclick="toggleDescription(12)">P13 ML Taxi Order Forecasting</div>
      <div class="portfolio-item" onclick="toggleDescription(13)">P14 Movie Rating by Sentimental Analysis</div>
      <div class="portfolio-item" onclick="toggleDescription(14)">P15 CNN Age Prediction</div>
      <div class="portfolio-item" onclick="toggleDescription(15)">P16 ML Unsupervised learning</div>
      <div class="portfolio-item" onclick="toggleDescription(16)">P17 ML Churn Prediction</div>
      <div class="portfolio-item" onclick="toggleDescription(17)">P18 ML Phishing Classification</div>
    </div>
    <div id="portfolio-desc" class="portfolio-desc">
        <div id="desc-0" class="desc-card" style="display: none;">
            <h5 class="project-title"><b>Project 1 - Data Cleaning</b></h5>
            <p>During my first project, I developed a data analysis project for Store 1, an e-commerce company that aims to better understand their customers' behavior through data analysis. The main objective was to evaluate and improve the quality of the collected data to make informed decisions that optimize the customer experience.</p>
            <p>First, I identified issues in the collected data, such as incorrect data types for age and user ID, unnecessary spaces in names, and inconsistent formatting of favorite categories. After recognizing these issues, I applied data cleaning techniques to correct them, ensuring a coherent and usable database for future analysis.</p>
            <p>Specifically, I transformed the user ID and age into appropriate integer values, removed unnecessary spaces, and adjusted names to a cleaner and more presentable format. Additionally, although I did not consider it necessary to change the categories to lowercase, I implemented a script to do so if the company requires it in the future.</p>
            <p>To handle situations where data type conversion could fail, I developed code that prevents system errors and prompts the user to provide an appropriate numerical value instead of unprocessable text.</p>
            <p>Additionally, I calculated key metrics on user spending habits, such as total, minimum, and maximum spending, and simulated new purchases to determine when a customer meets the loyalty criteria defined by the company.</p>
            <p>The final step of the project was to design a method to summarize and present user information effectively. This included creating formatted messages with essential data and organizing the information into a table that facilitates future consultation and analysis.</p>
            <p>In conclusion, this project not only improved the quality and coherence of Store 1's database but also provided crucial tools for data analysis and decision-making. The applied techniques will ensure that the company can better understand their customers and continuously improve their online shopping experience.</p>
            <p><a href="https://github.com/jmhrivera/project_01_store">Check out my repository</a></p>
        </div>

        <div id="desc-1" class="desc-card" style="display: none;">
            <h5 class="project-title"><b>Project 2 - Data Preprocessing</b></h5>
            <p>In this project, I aimed to compare musical preferences between the cities of Springfield and Shelbyville using real online music streaming data. The main objective was to test three hypotheses related to user activity and musical genres.</p>
            <p>The project began with importing and examining the dataset, which contained seven columns: user ID, song title, artist, genre, city, time, and day. Initial data exploration revealed issues like inconsistent headers, missing values in 'Track', 'artist', and 'genre', and 3826 duplicate entries.</p>
            <p>I addressed these issues by standardizing the headers, replacing missing values with 'unknown', and removing explicit duplicates. For implicit duplicates, particularly within the 'genre' column, I ensured names were consistent to maintain data accuracy. This preprocessing ensured the dataset was clean and ready for analysis.</p>
            <p>For hypothesis testing:</p>
            <ol>
                <li>I found that user activity peaks on different days in each city, confirming that Springfield sees more activity on Fridays, while Shelbyville peaks on Wednesdays.</li>
                <li>When comparing musical preferences on Monday mornings and Friday evenings, I discovered only minor differences, leading to the rejection of this hypothesis.</li>
                <li>The analysis showed that pop was the most preferred genre in both cities, with rap not making the top 10 in either city, thus rejecting the hypothesis that Springfield prefers pop and Shelbyville prefers rap.</li>
            </ol>
            <p>In conclusion, my analysis revealed different patterns of user activity between the two cities but similar musical genre preferences, with pop being the top choice in both. The project underscored the importance of thorough data preprocessing and provided valuable insights into user behavior that can inform business decisions.</p>
            <p><a href="https://github.com/jmhrivera/project_02_music_preferences">Check out my repository</a></p>
        </div>

        <div id="desc-2" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 3 - Extract Data Analysis (EDA)</b></h5>
          <p>During my project, I developed a data analysis project for Instacart, an e-commerce grocery delivery platform aiming to better understand customer behavior through data analysis. The main objective was to evaluate and improve the quality of the collected data for informed decision-making that optimizes the customer experience.</p>
          <p>First, I identified issues in the dataset, such as missing values in columns like "product_name" and "days_since_prior_order," duplicated entries in the "instacart_orders" data frame, and inconsistent formatting across different fields. I applied data cleaning techniques to correct these issues, ensuring a coherent and usable database for future analysis.</p>
          <p>I standardized column names and data types, replaced missing values with appropriate placeholders (e.g., 'unknown'), and removed duplicated entries. For example, I found that 15 orders had been duplicated, likely due to a post-capture error. Additionally, corrected the "add_to_cart_order" column by identifying and addressing entries where orders had more than 64 products, which presented as null values due to a system error.</p>
          <p>Next, I analyzed the data to extract key insights. I created bar charts to visualize customer ordering behavior by hour and day of the week, finding that most orders are placed between 10:00 AM and 4:00 PM, especially on Sundays, Mondays, and Tuesdays. I also examined how long customers typically wait between orders, with most reordering within 10 days.</p>
          <p>I further explored product-specific data, identifying the top 20 most frequently ordered products, which primarily included fruits and vegetables. I calculated the reorder rates for each product and customer, revealing interesting patterns in customer loyalty and purchasing habits. For instance, bananas were found to be the most reordered product, and customers showed a 49% average reorder ratio, indicating significant repeat purchases.</p>
          <p>Finally, I identified the top 20 items customers add first to their cart, highlighting that water, soda, and half & half were commonly added first. This analysis provided actionable insights into customer preferences and behaviors.</p>
          <p>In conclusion, this project improved the quality and coherence of Instacart's database and provided crucial tools for data analysis and decision-making. The applied techniques and findings will help Instacart better understand their customers and continually improve their online shopping experience.</p>
          <p><a href="https://github.com/jmhrivera/project_03_Instacart_Analysis">Check out my repository</a></p>
        </div>

        <div id="desc-3" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 4 - Hypothesis Testing</b></h5>
          <p>During my recent project, I conducted a comprehensive analysis comparing two mobile plans, Surf and Ultimate, to assess their effectiveness and customer satisfaction. The primary objective was to analyze usage patterns and revenue generated by each plan to determine if the current offerings meet customer needs and to explore potential areas for improvement.</p>
          <p>First, I analyzed the monthly call duration data for both plans. I found that the average call duration per month was similar for both plans, with Surf users averaging 428.8 minutes and Ultimate users averaging 430.45 minutes. However, the variance and standard deviation were slightly higher for Ultimate users, indicating a wider range of call durations among this group. Despite the similarities, a significant number of Surf users exceeded their plan's limits, suggesting that the Surf plan may not fully satisfy the needs of its users.</p>
          <p>Next, I examined the number of messages sent each month by users of each plan. The analysis revealed that although both groups generally stayed within their plan limits, Ultimate users tended to send more messages on average compared to Surf users. The data also showed a positive skew in the distribution, with a significant portion of Surf users exceeding their plan's 50-message limit, leading to additional charges.</p>
          <p>I then compared the internet data usage for both plans. The findings showed that, starting in the second half of the year, Surf users began to exceed their 15GB data limit on average, while Ultimate users, with a 30GB limit, rarely surpassed their cap. The analysis indicated that the Surf plan users often consumed more data than their plan allowed, resulting in additional charges and potential dissatisfaction.</p>
          <p>Furthermore, I evaluated the revenue generated by each plan. Although the Surf plan had twice as many users as the Ultimate plan, the Ultimate plan generated higher revenue due to the additional charges incurred by users exceeding their plan limits. This trend was consistent throughout the year, with the gap narrowing slightly by December.</p>
          <p>To statistically test these observations, I conducted hypothesis tests to compare the average revenue generated by users of the two plans and to evaluate whether the revenue from the NY-NJ area differed from other regions. The results confirmed that the average revenue differs significantly between the Surf and Ultimate plans, and between the NY-NJ region and other regions for the Ultimate plan. However, no significant difference was found for the Surf plan across different regions.</p>
          <p>In conclusion, this project highlighted the potential shortcomings of the Surf plan in meeting customer needs, as evidenced by the frequent exceeding of plan limits for calls, messages, and data. This could lead to customer dissatisfaction and an increased risk of churn. To mitigate this, it may be necessary to reassess the Surf plan's offerings or encourage users to switch to the Ultimate plan. Additionally, the analysis underscored the importance of continually monitoring user behavior and adjusting plan features to better align with customer expectations and market trends.</p>
          <p><a href="https://github.com/jmhrivera/project_04_Telecom_Tariff_Analysis">Check out my repository</a></p>
        </div>

        <div id="desc-4" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 5 - Streamlit Dashboard</b></h5>
          <p>During my project, I developed a data analysis project for used car sales. The objective was to create an interactive dashboard that stakeholders can use to explore various aspects of the car sales dataset, such as average prices, vehicle conditions, and other significant factors. The project involved using various data visualization techniques to present insights about vehicle conditions, prices, and other attributes via a Streamlit-based interactive dashboard.</p>
          <p>First, I imported and cleaned the car sales data. This involved loading the data into a DataFrame from a CSV file, removing rows with missing values in the 'model_year' column, and converting columns to appropriate data types, such as converting 'date_posted' to datetime format. These preprocessing steps ensured that the dataset was ready for analysis and visualization.</p>          
          <p>Next, I implemented an interactive dashboard using Streamlit. The dashboard allows users to filter and visualize car sales data based on several criteria like model year, car type, fuel type, and the number of cylinders. This interactive feature enables stakeholders to tailor the data views according to their specific needs and preferences.</p>          
          <p>The dashboard includes various data visualizations such as bar charts, histograms, box plots, and scatter plots. These visualizations help derive meaningful insights into the dataset, such as the average prices of cars, their conditions, and odometer readings. By toggling between different views, users can gain a comprehensive understanding of the factors that influence car sales.</p>          
          <p>Additionally, I ensured that the data visualizations are user-friendly and provide key insights about the cars. For instance, bar charts can show the distribution of car types, while scatter plots can depict the relationship between car prices and odometer readings. These visualizations facilitate data-driven decision-making for stakeholders.</p>          
          <p>In conclusion, this project resulted in an interactive dashboard that effectively visualizes the used car sales dataset. The dashboard not only improved the usability of the dataset but also provided valuable insights into vehicle conditions, prices, and other attributes. The applied techniques will assist stakeholders in better understanding the car market and making informed decisions regarding car sales.</p>   
          <p>Applink: <a href="https://car-sales-vm0b.onrender.com">Car Sales Dashboard</a></p>
        </div>

        <div id="desc-5" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 6 - Gaming API database</b></h5>
          <p>During this project, I developed a data analysis workflow to process and clean a dataset containing information about various games. The main objective was to ensure that the data was accurate, consistent, and ready for further analysis.</p>
          <p>First, I imported the necessary libraries, including pandas, numpy, matplotlib, plotly, and seaborn. The dataset was then loaded into a DataFrame using pandas.</p>
          <p>In the data processing step, I followed a systematic approach:</p>
          <p>Data Examination: I thoroughly examined the dataset to identify any missing values, outliers, duplicates, and other anomalies that could affect the analysis.</p>
          <p>Handling Missing Data: A strategy was established to manage missing data, either by removing rows with missing values, imputing missing data, or considering advanced imputation methods.</p>
          <p>Duplicate Removal: Any duplicate rows were identified and removed to prevent duplication of information.</p>
          <p>Inconsistency Correction: I identified and corrected inconsistencies in the data, such as fixing typos, standardizing measurement units, and addressing other discrepancies.</p>
          <p>Outlier Management: Outliers were appropriately handled—either removed, transformed, or treated according to the context.</p>
          <p>Consistency in Data Formats: Ensured that data formats, such as dates and categorical variables, were consistent across the dataset.</p>
          <p>This project improved the dataset's quality, making it suitable for further analysis and helping ensure that the insights derived from it would be reliable and accurate.</p>
          <p><a href="https://github.com/jmhrivera/project_06_VideoGame_Sales_Analysis">Check out my repository</a></p>
        </div>

        <div id="desc-6" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 7 - Hypothesis Testing</b></h5>
          <p>During my first project, I focused on analyzing cab company data to understand customer behavior better and optimize services. The main objective was to evaluate and improve the data quality and derive meaningful insights using statistical techniques.</p>
          <p>I imported data from three different dataframes capturing information about cab trips, drop-offs, and weather conditions. Initial exploratory data analysis (EDA) helped identify and address duplicates and inconsistencies, ensuring clean data ready for analysis.</p>
          <p>The "Trips" dataset required minimal cleaning and revealed that `Flash Cab` held a 19.7% market share among the top ten cab companies. This insight was crucial for understanding market dynamics within the dataset.</p>
          <p>The "Drop-offs" dataset was categorized by average trips across various neighborhoods. "Loop" had the highest average trips, highlighting it as a significant area for cab services, followed by "River North" and "Streeterville."</p>
          <p>The hypothesis tested was whether the average trip lengths from Loop to The International Airport of O'Hare change on rainy Saturdays. Using statistical tests like the Levene test for variances and the T-test for means, we found enough evidence to reject the null hypothesis, indicating a difference based on weather conditions.</p>
          <p>This project not only improved the data quality of the cab databases but also provided significant analytical insights. The techniques used, including hypothesis testing and exploratory data analysis, furnish the cab companies with tools for better decision making and enhancing customer experience.</p>
          <p>The structured approach and detailed coding, including handling statistical tests, underscore the importance of clean and well-analyzed data in deriving meaningful and actionable insights.</p>
          <p><a href="https://github.com/jmhrivera/project_07_Cab_Analysis_And_Hypothesis_Testing">Check out my repository</a></p>
        </div>

        <div id="desc-7" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 8 - ML Classification</b></h5>
          <p>During this project, I developed a machine learning model for Megaline, a telecommunications company, to analyze customer behavior and recommend data plans. The main objective was to use this model to categorize plans into either "Smart" or "Ultra."</p>
          <p>First, I loaded the necessary data and conducted an exploratory analysis. The dataset was clean overall but required some minor adjustments, such as converting the data types for certain columns like calls and messages. After assessing the data quality and ensuring its suitability for modeling, I segmented it into training, validation, and testing datasets.</p>
          <p>I explored three different classification algorithms to find the best model: Decision Tree, Random Forest, and Logistic Regression. Each model was painstakingly tuned to optimize its performance. For the Decision Tree and Random Forest, I iterated through a range of tree depths and estimator counts to determine the best fit for our data.</p>
          <p>The Random Forest model emerged as the most effective, exhibiting substantial improvements over the Decision Tree and Logistic Regression models. It not only had the highest overall accuracy but also excelled in precision, recall, and F1 score, particularly in identifying true positives.</p>
          <p>To conclude, this project effectively utilized data analysis and machine learning techniques to provide Megaline with a robust tool for making data-driven decisions about customer data plans. The Random Forest model, with its superior performance metrics, was recommended as the best option for deployment to help the company understand customer behavior and optimize their data plan offerings.</p>
          <p>With these techniques we've ensured that the company can better understand their customers and therefore continuously improve their services.</p>
          <p><a href="https://github.com/jmhrivera/project_08_ML_Telecom_Classification">Check out my repository</a></p>
        </div>

        <div id="desc-8" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 9 - ML Churn Prediction</b></h5>
          <p>During this project, I developed a machine learning model for Beta Bank to predict whether customers would leave the bank soon. The primary goal was to create a model with the highest possible F1 score to ensure accurate prediction.</p>
          <p>First, I preprocessed the data by conducting an exploratory analysis. The dataset required minor adjustments such as encoding categorical variables and filling null values. After cleaning the data, I split it into training and testing datasets.</p>
          <p>I tested three classification algorithms: Random Forest, Support Vector Machine (SVM), and Logistic Regression. Using GridSearchCV, I performed hyperparameter tuning for each model, experimenting with different configurations to achieve the best performance.</p>
          <p>Among the models, the Random Forest and SVM yielded the best results, with comparable performance metrics. The evaluation metrics included accuracy, precision, recall, and F1 score, with particularly close results in recall and precision.</p>
          <p>However, the Random Forest model slightly outperformed the SVM in terms of the Area Under the Receiver Operating Characteristic Curve (AUROC). Given the emphasis on precision in predicting which customers might leave, I recommended the Random Forest model as the optimal choice for deployment.</p>
          <p>In conclusion, this project successfully utilized data analysis and machine learning techniques to provide Beta Bank with a robust tool for predicting customer churn. The Random Forest model was chosen for its superior ability to identify at-risk customers, thereby enabling the bank to implement retention strategies effectively.</p>
          <p><a href="https://github.com/jmhrivera/project_09_ML_Bank_Customer_Churn_Prediction">Check out my repository</a></p>
        </div>

        <div id="desc-9" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 10 - ML Oil Prediction</b></h5>
          <p>During this project, I developed a predictive model for OilyGiant to identify the optimal locations for 200 new oil wells. The primary goal was to accurately estimate the volume of reserves and identify the most profitable sites.</p>
          <p>Firstly, I conducted an exploratory data analysis on data from three different regions. This dataset required adjustments, such as correcting data types and handling any textual anomalies. Data visualization, particularly using histograms, helped detect any abnormalities, especially in Region 2's data.</p>
          <p>The next step was training a linear regression model, which involved splitting the data into training and testing sets with a 75% to 25% ratio. Model performance was evaluated using Root Mean Squared Error (RMSE). Region 1 data exhibited a normal distribution with good RMSE scores, while Region 2 data were skewed and potentially problematic.</p>
          <p>To assess profitability, I calculated the reserve volumes and potential profits of the top 200 wells in each region. The bootstrap method was used to analyze average profit, confidence intervals, and risk of loss. Regions 1 and 3 showed high profitability and low risk, whereas Region 2 had inconsistencies.</p>
          <p>Based on profitability metrics, Region 1 emerged as the best choice due to its higher total reserve volumes and more stable profit margins compared to Region 3. This analysis provided valuable insights for maximizing investment returns.</p>
          <p>Finally, the project emphasized the importance of data preprocessing, including correcting data types and visualizing distributions, to ensure robust model performance. The comparison of profitability metrics across regions supported the decision-making process for OilyGiant.</p>
          <p>In conclusion, the project successfully integrated data preprocessing, machine learning, and visualization techniques to recommend optimal drilling locations. This comprehensive approach ensured accurate predictions and supported OilyGiant's efforts in making data-driven decisions for new oil wells.</p>
          <p><a href="https://github.com/jmhrivera/project_10_Linear_Regression_Oil_Well_Profit_Prediction">Check out my repository</a></p>
        </div>

        <div id="desc-10" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 11 - ML Insurance Prediction</b></h5>
          
          <p>During this project, I developed a machine learning model for Sure Tomorrow, an insurance company, to evaluate and resolve various tasks with the aid of machine learning techniques.</p>
          <p>Initially, I explored the data to find clients similar to a given client, which helps in marketing. I utilized k-Nearest Neighbors (kNN) for clustering based on features like age, gender, income, and family members. Data were scaled using the MaxAbsScaler to enhance model performance. The kNN method was tested with both Euclidean and Manhattan distance metrics on scaled and non-scaled data, revealing significant differences in results when the data were not scaled.</p>
          <p>Next, I addressed the probability predictions of whether a new client would receive insurance benefits, benchmarking kNN classification against a dummy model. This step involved evaluating models with their F1 Scores while varying k from 1 to 10 across original and scaled data. Scaling consistently improved model performance, underscoring the necessity of data scaling for classification tasks.</p>
          <p>For regression problems, I predicted the number of insurance benefits a new client might receive using linear regression. I built custom implementations for linear regression by applying matrix operations to determine weights. Both original and scaled data showed the scaled data had better RMSE and R² scores, confirming the importance of data scaling in regression models.</p>
          <p>To safeguard client data, I implemented data obfuscation by multiplying feature matrices with an invertible matrix P. This step ensured data protection without compromising model accuracy. The transformed data maintained robust RMSE and R² scores, proving that obfuscation does not degrade model performance. Testing with a class that can obfuscate and reverse data confirmed this approach’s efficacy.</p>
          <p>Finally, the project involved evaluating the performance metrics and making conclusive decisions for field deployment. Each phase justified the approaches and techniques used to ensure optimized and protected data analysis, supportive of Sure Tomorrow’s data-driven goals.</p>
          <p>In conclusion, this project effectively demonstrated how data preprocessing, scaling, regression, classification, and obfuscation techniques can be integrated to enhance decision-making processes. This approach ensures that Sure Tomorrow can derive valuable insights from their data while maintaining the integrity and confidentiality of client information.</p>          
          <p><a href="https://github.com/jmhrivera/project_11_ML_Customer_Insurance_Analysis">Check out my repository</a></p>
        </div>

        <div id="desc-11" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 12 - ML Car Price Prediction</b></h5>
          <p>During this project, I developed a predictive model for Rusty Bargain to determine the market value of used cars. The primary goal was to create a model with high prediction quality, fast prediction speed, and efficient training time.</p>
          <p>Initially, I performed data preprocessing, which involved handling missing values and correcting data types. For example, I opted to drop rows with null entries due to the significant weight of the dataset, which made imputations infeasible. I also used One-Hot Encoding (OHE) for categorical variables and scaled numerical features using StandardScaler.</p>
          <p>I trained and evaluated several machine learning models, including Linear Regression, Random Forest, LightGBM, CatBoost, and XGBoost. Parameters for these models were optimized using GridSearchCV, and their performance was measured using RMSE (Root Mean Squared Error). Both preprocessing steps and model training were performed on training and testing splits of the data, ensuring robust evaluation metrics.</p>
          <p>Linear Regression showed decent performance but was outperformed by ensemble methods like Random Forest and gradient boosting models. Random Forest provided better RMSE scores but took longer to train. Among the gradient boosting models, XGBoost achieved the best RMSE scores with an optimal prediction threshold around 1476 euros, followed closely by CatBoost and LightGBM.</p>
          <p>An essential part of the preprocessing involved identifying and mitigating biases in the data. For instance, price values below 600 euros were excluded to avoid unrealistic entries, and registration years outside the 1980-2016 range were filtered out. Power values were constrained between 60 and 300 CV to maintain consistency with real-world data.</p>
          <p>In addition to cleaning and transforming the data, I verified the presence of duplicate rows and removed them to ensure the dataset's integrity. This proactive approach helped improve the model's accuracy and reliability.</p>
          <p>The project's conclusion emphasized the significance of comprehensive data preprocessing, including the correct encoding of categorical variables and scaling of numerical values. The comparison of various machine learning models highlighted the superior performance of gradient boosting techniques, particularly XGBoost, in predicting car prices while maintaining efficiency in training and prediction times.</p>
          <p><a href="https://github.com/jmhrivera/project_12_GBM_Regression_Car_Price_Prediction">Check out my repository</a></p>
        </div>

        <div id="desc-12" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 13 - ML Taxi Forecasting</b></h5>
          <p>During this project, I developed a machine learning model for Sweet Lift Taxi to predict the number of taxi orders for the next hour. The main goal was to create a model with an RMSE not exceeding 48 on the test set.</p>
          <p>First, I conducted data preprocessing by handling missing values and ensuring the data was consistent. I resampled the data by one hour and checked for duplicates and incoherencies in the dataset. After that, I visualized the monthly, weekly, daily, and hourly data trends to understand the data better.</p>
          <p>I then trained several machine learning models including Linear Regression, Gradient Boosting, CatBoost, XGBoost, Auto ARIMA, and Prophet. These models were assessed using RMSE as the evaluation metric, with training and testing splits to ensure robust evaluation. The models were fine-tuned using GridSearchCV to optimize their performance.</p>
          <p>The CatBoost model demonstrated the best performance, achieving an RMSE of around 45, followed by XGBoost and Prophet. These results were achieved through careful preprocessing, data scaling, and hyperparameter tuning that ensured the models were accurately trained on the historical data.</p>
          <p>In the preprocessing phase, it was imperative to correct data types, fill missing values, and encode categorical variables appropriately. Additionally, I made use of data transformation techniques to maintain the integrity and consistency of the dataset for better model performance.</p>
          <p>To ensure model robustness and reduce potential overfitting, I implemented a time series cross-validation using a TimeSeriesSplit approach. This method allowed for a thorough evaluation and selection of the best-performing model based on its prediction accuracy.</p>
          <p>Finally, the project concluded with a comprehensive analysis of model performance. The results were compiled into a ranking table showcasing CatBoost as the top-performing model. The project validated the importance of extensive data preprocessing and rigorous model evaluation in achieving reliable predictions.</p>
          <p>In conclusion, this project demonstrated how effective data preprocessing combined with advanced machine learning techniques can address predictive modeling challenges. Sweet Lift Taxi now has a reliable tool to forecast taxi demand, which can improve driver availability during peak hours and enhance customer satisfaction.</p>
          <p><a href="https://github.com/jmhrivera/project_13_TimeSeries_Prediction_Ensemble_taxi_order_forecasting">Check out my repository</a></p>
        </div>

        <div id="desc-13" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 14 - Movie Rating by Sentimental Analysis</b></h5>
          <p>During this project, I developed several machine learning models to analyze customer reviews for Film Junky Union, focused on detecting negative movie reviews automatically. The objective was to achieve an F1 score of at least 0.85.</p>
          <p>Initially, I conducted data preprocessing, addressing missing values and ensuring the data's consistency. I applied One-Hot Encoding (OHE) for categorical variables and scaled numerical features using StandardScaler. Text data was normalized to lowercase and stripped of digits, punctuation, and stopwords using NLTK and spaCy tools.</p>
          <p>I tested several models, including Dummy Classifier, NLTK-based Logistic Regression, spaCy-based Logistic Regression, and BERT-based models. Models were evaluated using metrics like F1 score, accuracy, and ROC AUC. GridSearchCV was used to fine-tune model parameters, ensuring optimal performance.</p>
          <p>Among the models, NLTK-based Logistic Regression showed outstanding performance, achieving an F1 score of 0.88. This model outperformed the spaCy-based Logistic Regression and the initial Dummy Classifier. BERT models showed promising results, but their performance depended on the dataset size, suggesting potential improvements with larger training sets.</p>
          <p>I also analyzed the distribution of positive and negative reviews over the years, ensuring the dataset balance. This was crucial for the model's robustness and reliability during prediction. Additional steps included evaluating data for duplicates and null values and removing them to maintain data integrity.</p>
          <p>A significant part of the preprocessing phase involved text tokenization and vectorization using TF-IDF. This transformation of text data into numerical format was essential for training the models. The comparison between NLTK and spaCy showed that NLTK performed better with TF-IDF in Logistic Regression models.</p>
          <p>The project concluded with detailed model evaluations, compiling the results into a ranking table. NLTK-based Logistic Regression emerged as the top model due to its high F1 score and reliable performance across various metrics. This model is recommended for deployment in detecting negative movie reviews.</p>
          <p>In conclusion, this project demonstrated the importance of thorough data preprocessing, the effectiveness of different text processing techniques, and comprehensive model evaluation. These efforts provided Film Junky Union with a reliable tool for enhancing customer insights and improving review analysis processes.</p>
          <p><a href="https://github.com/jmhrivera/project_14_Multimodal_Model_Ensemble_sentiment_analysis">Check out my repository</a></p>
        </div>

        <div id="desc-14" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 15 - CNN Age Prediction</b></h5>
          <p>During this project, I developed a neural network model to estimate the age of individuals based on facial images for a company’s application. The primary objective was to achieve a low mean absolute error (MAE) by efficiently processing the image data.</p>
          <p>Initially, I managed data preprocessing, which included handling image data and labels. I created data generators using ImageDataGenerator from Keras to handle loading, augmenting, and rescaling the image data. These generators were essential for efficiently feeding the data into the neural network in batches.</p>
          <p>I defined the model architecture using the ResNet50 backbone pre-trained on ImageNet for feature extraction. This model was chosen for its robustness in handling image data. I added layers on top of ResNet50 for global average pooling and a dense layer to produce a single real-valued output. The model was compiled with the Adam optimizer using a learning rate of 0.0001 and mean squared error as the loss function.</p>
          <p>Training the model involved splitting the data into training and validation sets using a 75%-25% split. I implemented the training using 20 epochs and monitored the MAE for both training and validation sets to ensure the model's generalization capabilities. The training process included applying <code>%%time</code> to measure the execution time effectively.</p>
          <p>Model evaluation showed promising performance, with the final MAE on the validation set reaching around 7.2 years. This result satisfied the project's threshold. Additionally, the use of TimeSeriesSplit for cross-validation helped ensure the robustness of the results.</p>
          <p>A significant portion of the preprocessing involved cleaning the dataset by removing any duplicates and correcting any mismatches in the image-label pairs. This was crucial for maintaining data integrity and ensuring accurate model performance.</p>
          <p>Finally, the project's comprehensive analysis validated the importance of thorough data preprocessing and the utilization of a robust model such as ResNet50. Deploying this model will enhance the company's capability to estimate ages accurately based on facial images, thereby enriching their application’s functionality.</p>
          <p>In conclusion, this project successfully applied advanced data preprocessing and neural network techniques to create an effective age estimation model. The results demonstrated the model's capability in handling real-world image data, thus supporting the company’s business requirements efficiently.</p>
          <p><a href="https://github.com/jmhrivera/project_15_CNN_ResNet50_age_prediction">Check out my repository</a></p>
        </div>

        <div id="desc-15" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 16 - Machine Learning Unsupervised Learning</b></h5>
          <p>During this project, I developed a neural network model to estimate the age of individuals based on facial images for a company’s application. The primary objective was to achieve a low mean absolute error (MAE) by efficiently processing the image data.</p>
          <p>Initially, I managed data preprocessing, which included handling image data and labels. I created data generators using ImageDataGenerator from Keras to handle loading, augmenting, and rescaling the image data. These generators were essential for efficiently feeding the data into the neural network in batches.</p>
          <p>I defined the model architecture using the ResNet50 backbone pre-trained on ImageNet for feature extraction. This model was chosen for its robustness in handling image data. I added layers on top of ResNet50 for global average pooling and a dense layer to produce a single real-valued output. The model was compiled with the Adam optimizer using a learning rate of 0.0001 and mean squared error as the loss function.</p>
          <p>Training the model involved splitting the data into training and validation sets using a 75%-25% split. I implemented the training using 20 epochs and monitored the MAE for both training and validation sets to ensure the model's generalization capabilities. The training process included applying <code>%%time</code> to measure the execution time effectively.</p>
          <p>Model evaluation showed promising performance, with the final MAE on the validation set reaching around 7.2 years. This result satisfied the project's threshold. Additionally, the use of TimeSeriesSplit for cross-validation helped ensure the robustness of the results.</p>
          <p>A significant portion of the preprocessing involved cleaning the dataset by removing any duplicates and correcting any mismatches in the image-label pairs. This was crucial for maintaining data integrity and ensuring accurate model performance.</p>
          <p>Finally, the project's comprehensive analysis validated the importance of thorough data preprocessing and the utilization of a robust model such as ResNet50. Deploying this model will enhance the company's capability to estimate ages accurately based on facial images, thereby enriching their application’s functionality.</p>
          <p>In conclusion, this project successfully applied advanced data preprocessing and neural network techniques to create an effective age estimation model. The results demonstrated the model's capability in handling real-world image data, thus supporting the company’s business requirements efficiently.</p>
          <p><a href="https://github.com/jmhrivera/project_16_resnet_face_age_estimation">Check out my repository</a></p>
        </div>

        <div id="desc-16" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 17 - Machine Learning Churn Prediction</b></h5>
          <p>During this project, I focused on developing a machine learning model for customer churn prediction for an insurance company. The main objective was to manage data preprocessing, accurately predicting customer churn, and ensuring model robustness.</p>
          <p>First, I conducted data preprocessing by managing missing values and ensuring consistency in the dataset. I performed One-Hot Encoding (OHE) for categorical variables, and scaling of numerical features using StandardScaler. Missing values were handled using SimpleImputer and imputations were performed through machine learning models when necessary.</p>
          <p>I trained and evaluated several machine learning models, including Logistic Regression, XGBoost, LightGBM, and a simple neural network using Keras. Each model was carefully tuned using GridSearchCV to optimize hyperparameters, and performance was measured using evaluation metrics such as ROC AUC and accuracy. Training and testing datasets were created using an 80%-20% split to ensure robust evaluation.</p>
          <p>The logistic regression model achieved a good balance between accuracy and computational efficiency. However, the XGBoost model outperformed others in terms of ROC AUC, making it the best candidate for predicting customer churn. LightGBM also showed promising results but was slightly less effective compared to XGBoost. The neural network model also provided decent performance but required more computational resources and longer training times.</p>
          <p>A critical part of the preprocessing involved ensuring that categorical variables were correctly encoded and numerical values were appropriately scaled. This preprocessing step was vital for enhancing model performance and prediction accuracy.</p>
          <p>Cross-validation using TimeSeriesSplit was implemented to further validate model robustness, ensuring that the models performed well on different time segments of the data. Oversampling techniques such as SMOTE were also explored to handle class imbalance in the target variable, though the logistic regression model did not require significant oversampling.</p>
          <p>Finally, the project concluded with a comprehensive analysis comparing all models' performances. The results indicated that the XGBoost model was the top performer, followed closely by LightGBM and Logistic Regression. These findings provided actionable insights for deploying a reliable churn prediction model within the insurance company.</p>
          <p>In summary, this project successfully applied advanced preprocessing techniques and various machine learning models to predict customer churn. The results affirmed the importance of thorough preprocessing and model evaluation, providing the insurance company with a robust solution to enhance customer retention strategies.</p>
          <p><a href="https://github.com/jmhrivera/project_17_customer_churn_prediction">Check out my repository</a></p>
        </div>

        <div id="desc-17" class="desc-card" style="display: none;">
          <h5 class="project-title"><b>Project 18 - Phishing Classification</b></h5>
          <p>During this project, I developed a machine learning model to predict phishing websites from legitimate ones. The main goal was to achieve high accuracy and robustness in identifying phishing attempts based on various extracted features.</p>
          <p>First, data preprocessing was essential. I managed missing values, ensured data consistency, and transformed categorical variables using One-Hot Encoding (OHE). For numerical features, I applied scaling techniques using StandardScaler to normalize the data. This step was paramount in preparing the data for the machine learning models.</p>
          <p>I experimented with several machine learning models, including Logistic Regression and LightGBM. These models were evaluated using metrics like ROC AUC and accuracy. GridSearchCV was used to fine-tune hyperparameters, ensuring optimal model performance. The training involved an 80%-20% split for training and testing, respectively.</p>
          <p>Logistic Regression performed decently, providing a good balance between accuracy and computational efficiency. However, the LightGBM model outperformed others, achieving superior results in ROC AUC metrics. This made LightGBM the best candidate for deployment in identifying phishing websites.</p>
          <p>Key preprocessing steps included handling outliers and ensuring that categorical values were correctly encoded. These steps were critical for enhancing the model's predictive accuracy and overall performance. Effective transformation and selection of features played an essential role in the success of this phase.</p>
          <p>To ensure the robustness of the model, I employed cross-validation techniques, specifically using TimeSeriesSplit. This method verified the model’s consistency across different segments of the data. Additionally, I explored techniques like SMOTE to address class imbalance in the target variable, though Logistic Regression handled class balance well even without significant oversampling.</p>
          <p>The project concluded with a thorough evaluation, comparing all the models’ performances. The LightGBM model emerged as the top performer with the highest ROC AUC score, followed by Logistic Regression. These findings provided the basis for recommending the LightGBM model for production deployment.</p>
          <p>In summary, this project demonstrated the importance of comprehensive data preprocessing and robust model evaluation in building effective machine learning solutions for phishing detection. The insurance company now has a reliable tool to enhance its cybersecurity measures, detecting phishing websites with high accuracy and efficiency.</p>
          <p><a href="https://github.com/jmhrivera/project_18_Phishing_Exploration">Check out my repository</a></p>
        </div>


        <!-- Otros proyectos -->
    </div>
</div>


<script src="js/portfolio.js"></script> <!-- Archivo JS externo -->
<!-- ___________________________ -->


    <!-- THIS SECTION WAS HIDE BY ME BECAUSE I WANTED A DIFFERENT DESIGN -->
    <!-- <div class="tab-content gallery mt-5"> -->
      <!-- <div class="tab-pane active" id="web-development">

        <div class="ml-auto mr-auto">
          <div class="row">
            <div class="col-md-6">
              <div class="cc-porfolio-image img-raised" data-aos="fade-up" data-aos-anchor-placement="top-bottom"><a href="https://jmhrivera.github.io/portfolio"  rel="nofollow" target="_blank">
                  <figure class="cc-effect"><img src="images/imagen-a.webp" alt="Image"/>
                    <figcaption>
                      <div class="h4">Pronóstico de Ventas en Volumen</div>
                      <p>Training</p>
                    </figcaption>
                  </figure></a></div>
              <div class="cc-porfolio-image img-raised" data-aos="fade-up" data-aos-anchor-placement="top-bottom"><a href="https://jesusbz.github.io"  rel="nofollow" target="_blank">
                  <figure class="cc-effect"><img src="images/imagen-ll.webp" alt="Image"/>
                    <figcaption>
                      <div class="h4">Evaluación de Proyecto</div>
                      <p>Proyecto de Inversión</p>
                    </figcaption>
                  </figure></a></div>
            </div>
            <div class="col-md-6">
              <div class="cc-porfolio-image img-raised" data-aos="fade-up" data-aos-anchor-placement="top-bottom"><a href="https://jesusbz.github.io"  rel="nofollow" target="_blank">
                  <figure class="cc-effect"><img src="images/imagen-c.webp" alt="Image"/>
                    <figcaption>
                      <div class="h4">Machine Learning</div>
                      <p>Forecast</p>
                    </figcaption>
                  </figure></a></div>
              <div class="cc-porfolio-image img-raised" data-aos="fade-up" data-aos-anchor-placement="top-bottom"><a href="https://jesusbz.github.io"  rel="nofollow" target="_blank">
                  <figure class="cc-effect"><img src="images/imagen-e.webp" alt="Image"/>
                    <figcaption>
                      <div class="h4">Financial Dashboard</div>
                      <p>Intelligence Business</p>
                    </figcaption>
                  </figure></a></div>
            </div>
          </div>
        </div>
      </div>
      <div class="tab-pane" id="graphic-design" role="tabpanel">
        <div class="ml-auto mr-auto">
          <div class="row">
            <div class="col-md-6">
              <div class="cc-porfolio-image img-raised" data-aos="fade-up" data-aos-anchor-placement="top-bottom"><a href="https://jesusbz.github.io"  rel="nofollow" target="_blank">
                  <figure class="cc-effect"><img src="images/imagen-d.webp" alt="Image"/>
                    <figcaption>
                      <div class="h4">Portafolio de  Inversión</div>
                      <p>Markowitz</p>
                    </figcaption>
                  </figure></a></div>
              <div class="cc-porfolio-image img-raised" data-aos="fade-up" data-aos-anchor-placement="top-bottom"><a href="https://jesusbz.github.io"  rel="nofollow" target="_blank">
                  <figure class="cc-effect"><img src="images/imagen-b.webp" alt="Image"/>
                    <figcaption>
                      <div class="h4">Volatilidad y VaR del Bitcoin</div>
                      <p>Modelo y Pronósticos</p>
                    </figcaption>
                  </figure></a></div>
            </div>
            <div class="col-md-6">
              <div class="cc-porfolio-image img-raised" data-aos="fade-up" data-aos-anchor-placement="top-bottom"><a href="https://jesusbz.github.io"  rel="nofollow" target="_blank">
                  <figure class="cc-effect"><img src="images/imagen-g.webp" alt="Image"/>
                    <figcaption>
                      <div class="h4">Data Analytics</div>
                      <p>En desarrollo</p>
                    </figcaption>
                  </figure></a></div>
              <div class="cc-porfolio-image img-raised" data-aos="fade-up" data-aos-anchor-placement="top-bottom"><a href="https://jesusbz.github.io"  rel="nofollow" target="_blank">
                  <figure class="cc-effect"><img src="images/imagen-h.webp" alt="Image"/>
                    <figcaption>
                      <div class="h4">Forecasting Training</div>
                      <p>En desarrollo</p>
                    </figcaption>
                  </figure></a></div>
            </div>
          </div>
        </div>
      </div>
      <div class="tab-pane" id="Photography" role="tabpanel">
        <div class="ml-auto mr-auto">
          <div class="row">
            <div class="col-md-6">
              <div class="cc-porfolio-image img-raised" data-aos="fade-up" data-aos-anchor-placement="top-bottom"><a href="https://jesusbz.github.io"  rel="nofollow" target="_blank">
                  <figure class="cc-effect"><img src="images/imagen-i.webp" alt="Image"/>
                    <figcaption>
                      <div class="h4">En Diseño</div>
                      <p>Time Series</p>
                    </figcaption>
                  </figure></a></div>
              <div class="cc-porfolio-image img-raised" data-aos="fade-up" data-aos-anchor-placement="top-bottom"><a href="https://jesusbz.github.io"  rel="nofollow" target="_blank">
                  <figure class="cc-effect"><img src="images/imagen-j.webp" alt="Image"/>
                    <figcaption>
                      <div class="h4">En Diseño</div>
                      <p>Forecasting</p>
                    </figcaption>
                  </figure></a></div>
            </div>
            <div class="col-md-6">
              <div class="cc-porfolio-image img-raised" data-aos="fade-up" data-aos-anchor-placement="top-bottom"><a href="https://jesusbz.github.io"  rel="nofollow" target="_blank">
                  <figure class="cc-effect"><img src="images/imagen-k.webp" alt="Image"/>
                    <figcaption>
                      <div class="h4">En Diseño</div>
                      <p>Dashboard</p>
                    </figcaption>
                  </figure></a></div>
              <div class="cc-porfolio-image img-raised" data-aos="fade-up" data-aos-anchor-placement="top-bottom"><a href="https://jesusbz.github.io"  rel="nofollow" target="_blank">
                  <figure class="cc-effect"><img src="images/imagen-l.webp" alt="Image"/>
                    <figcaption>
                      <div class="h4">En Diseño</div>
                      <p>Big Data</p>
                    </figcaption>
                  </figure></a></div>
            </div>
          </div>
        </div>
      </div> -->
 
    </div>
  </div>
</div>

<!-- # EXPERIENCE SECTION -->
<div class="section" id="experience">
  <div class="container cc-experience">
    <div class="h4 text-center mb-4 title">Profesional Experience</div>
    <div class="card">
      <div class="row">
        <div class="col-md-3 bg-primary" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
          <div class="card-body cc-experience-header">
            <p>Jan 2018 - Current</p>
            <div class="h5">The Nielsen Company</div>
          </div>
        </div>
        <div class="col-md-9" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
          <div class="card-body">
            <div class="h5">Technical Account Manager</div>
            <p ALIGN="justify">
              As a Technical Account Manager, I provide customer training and support for various digital solutions focused on analyzing campaign performance. I serve as the primary technical post-sales contact for key customers, assisting them in achieving their goals by increasing revenue and retention. I bridge the communication gap between technical complexity and business requirements through simple and comprehensive solutions. Additionally, I extract, process, and manage databases to create analysis reports using tools such as Python, SQL, Google Sheets, and Excel.
            </p>
          </div>
        </div>
      </div>
    </div>
    <div class="card">
      <div class="row">
        <div class="col-md-3 bg-primary" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
          <div class="card-body cc-experience-header">
            <p>Sep 2015 - Feb. 2017</p>
            <div class="h5">La Jornada</div>
          </div>
        </div>
        <div class="col-md-9" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
          <div class="card-body">
            <div class="h5">Manager of Business Intelligence</div>
            <p ALIGN="justify">
              As a Manager of Business Intelligence, I managed the commercial intelligence area, overseeing strategic initiatives to align with business goals. I led a team of six, coordinating their activities to achieve set objectives. I formulated KPIs for both online and offline platforms, including metrics like website traffic, engagement rates, SEO performance for digital channels, and circulation, ad performance, reader engagement, and sales metrics for print to drive advertising effectiveness. I extracted and analyzed data using statistical software such as Ipsos and ComScore to derive actionable insights. I implemented and managed a comprehensive business dashboard in Power BI for daily data updates, presenting key insights to company managers. I also prepared and presented monthly detailed reports and charts with key industry information to the board of directors, and provided industry-specific metrics to the sales team to support their sales efforts.
            </p>
          </div>
        </div>
      </div>
    </div>  
    <div class="card">
      <div class="row">
        <div class="col-md-3 bg-primary" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
          <div class="card-body cc-experience-header">
            <p>Jan 2015 - Sep 2015</p>
            <div class="h5">Grupo Expansion</div>
          </div>
        </div>
        <div class="col-md-9" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
          <div class="card-body">
            <div class="h5">Business Intelligence Analyst</div>
            <p ALIGN="justify">
              As a Business Intelligence Analyst, I extracted and analyzed data using statistical tools like Ipsos and ComScore to derive actionable insights for the organization. I identified high-advertising industries through data analysis, targeting specific sectors and aligning objectives across 13 sites. I provided key industry-specific metrics and reports to the sales team to support their advertising sales efforts and tailor solutions to clients. Additionally, I developed and managed offline and online surveys to collect crucial data and enhance market understanding. I tracked the performance of 13 sites using metrics such as unique users and other key indicators.
            </p>
          </div>
        </div>
      </div>
    </div>    
  </div>
</div>

<!-- EDUCATION SECTION -->
<div class="section">
  <div class="container cc-education">
    <div class="h4 text-center mb-4 title">Education</div>

    <div class="card">
      <div class="row">
        <div class="col-md-3 bg-primary" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
          <div class="card-body cc-education-header">
            <p>Jul 2024 - Sep 2024</p>
            <div class="h5">Tecnológico Nacional de México</div>
          </div>
        </div>
        <div class="col-md-9" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
          <div class="card-body">
            <!-- <div class="h5">Data Science Diploma</div> -->
            <p class="category">Data Science Diploma</p>
            <p ALIGN="justify">
              At Tecnológico Nacional de México, I gained expertise in data science and Big Data. My coursework included descriptive and inferential statistics, as well as data exploration, grouping, and functions. I trained in Python programming, focusing on data structures and control flow. Furthermore, I developed skills in data manipulation and visualization using Pandas, NumPy, and Matplotlib. My training also covered machine learning, including techniques using Scikit-learn and neural networks.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="card">
      <div class="row">
        <div class="col-md-3 bg-primary" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
          <div class="card-body cc-education-header">
            <p>Oct 2023 - Jul 2024</p>
            <div class="h5">Tripleten (Bootcamp Online)</div>
          </div>
        </div>
        <div class="col-md-9" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
          <div class="card-body">
            <!-- <div class="h5">Data Science Diploma</div> -->
            <p class="category">Data Science Certificate</p>
            <p ALIGN="justify">
              During my Data Science Certificate program at Tripleten (Bootcamp Online), which I finalized, I mastered Python programming, covering variables, data types, loops, functions, and data analysis with Pandas. I learned advanced Python concepts like dictionaries, conditional statements, and data preprocessing. I developed skills in data wrangling, including handling missing values, filtering, visualizing data with Matplotlib, and feature engineering. My training included conducting statistical analysis such as descriptive statistics, probability theory, and hypothesis testing using Python. I also familiarized myself with software development tools like the command line, Git, and GitHub for version control and collaboration. I gained practical experience with SQL, focusing on data retrieval and advanced features. I applied machine learning techniques, including model training, evaluation, and regression using Scikit-learn and PyTorch. Additionally, I studied advanced machine learning topics such as supervised learning, classification metrics, handling imbalanced data using Scikit-learn, and deep learning with Keras and BERT. I completed capstone projects that integrated skills in data analysis and machine learning, including projects utilizing Keras and BERT for natural language processing and practical applications in business scenarios.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="card">
      <div class="row">
        <div class="col-md-3 bg-primary" data-aos="fade-right" data-aos-offset="50" data-aos-duration="500">
          <div class="card-body cc-education-header">
            <p>Feb 2022 - Jul 2022</p>
            <div class="h5">Google</div>
          </div>
        </div>
        <div class="col-md-9" data-aos="fade-left" data-aos-offset="50" data-aos-duration="500">
          <div class="card-body">
            <!-- <div class="h5">Data Science Diploma</div> -->
            <p class="category">Data Analyst Certificate</p>
            <p ALIGN="justify">
              I earned a Data Analyst Certificate from Google via Coursera, finalizing my training in foundational data analysis concepts, data-driven decision-making, and the role of data analysts. I learned to define and connect data-informed business decisions, as well as organize, load, and cleanse data effectively. My training included data cleaning techniques using SQL and performing data analysis with spreadsheets and SQL. I also studied the principles of data visualization and practiced creating visualizations using Tableau. Additionally, I gained a basic understanding of R programming for data analysis, culminating in the application of these skills in a final capstone project.
            </p>
          </div>
        </div>
      </div>
    </div>


  </div>
</div>

<!-- REFERENCES -->
<!-- <div class="section" id="reference">
  <div class="container cc-reference">
    <div class="h4 mb-4 text-center title">Referencias</div>
    <div class="card" data-aos="zoom-in">
      <div class="carousel slide" id="cc-Indicators" data-ride="carousel">
        <ol class="carousel-indicators">
          <li class="active" data-target="#cc-Indicators" data-slide-to="0"></li>
          <li data-target="#cc-Indicators" data-slide-to="1"></li>
          <li data-target="#cc-Indicators" data-slide-to="2"></li>
        </ol>
        <div class="carousel-inner">
          <div class="carousel-item active">
            <div class="row">
              <div class="col-lg-2 col-md-3 cc-reference-header"><img src="images/reference-image-1.jpg" alt="Image"/>
                <div class="h5 pt-2">Aiyana</div>
                <p class="category">CEO / WEBM</p>
              </div>
              <div class="col-lg-10 col-md-9">
                <p> Habitasse venenatis commodo tempor eleifend arcu sociis sollicitudin ante pulvinar ad, est porta cras erat ullamcorper volutpat metus duis platea convallis, tortor primis ac quisque etiam luctus nisl nullam fames. Ligula purus suscipit tempus nascetur curabitur donec nam ullamcorper, laoreet nullam mauris dui aptent facilisis neque elementum ac, risus semper felis parturient fringilla rhoncus eleifend.</p>
              </div>
            </div>
          </div>
          <div class="carousel-item">
            <div class="row">
              <div class="col-lg-2 col-md-3 cc-reference-header"><img src="images/reference-image-2.jpg" alt="Image"/>
                <div class="h5 pt-2">Braiden</div>
                <p class="category">CEO / Creativem</p>
              </div>
              <div class="col-lg-10 col-md-9">
                <p> Habitasse venenatis commodo tempor eleifend arcu sociis sollicitudin ante pulvinar ad, est porta cras erat ullamcorper volutpat metus duis platea convallis, tortor primis ac quisque etiam luctus nisl nullam fames. Ligula purus suscipit tempus nascetur curabitur donec nam ullamcorper, laoreet nullam mauris dui aptent facilisis neque elementum ac, risus semper felis parturient fringilla rhoncus eleifend.</p>
              </div>
            </div>
          </div>
          <div class="carousel-item">
            <div class="row">
              <div class="col-lg-2 col-md-3 cc-reference-header"><img src="images/reference-image-3.jpg" alt="Image"/>
                <div class="h5 pt-2">Alexander</div>
                <p class="category">CEO / Webnote</p>
              </div>
              <div class="col-lg-10 col-md-9">
                <p> Habitasse venenatis commodo tempor eleifend arcu sociis sollicitudin ante pulvinar ad, est porta cras erat ullamcorper volutpat metus duis platea convallis, tortor primis ac quisque etiam luctus nisl nullam fames. Ligula purus suscipit tempus nascetur curabitur donec nam ullamcorper, laoreet nullam mauris dui aptent facilisis neque elementum ac, risus semper felis parturient fringilla rhoncus eleifend.</p>
                  
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div> -->

<!-- CONCTACT SECTION -->
<div class="section" id="contact">
  <div class="cc-contact-information" style="background-image: url('images/map-1.webp');">
    <div class="container">
      <div class="cc-contact">
        <div class="row">
          <div class="col-md-9">
            <div class="card mb-0" data-aos="zoom-in">
              <div class="h4 text-center title">Contact</div>
              <div class="row">
                <div class="col-md-6">
                  <div class="card-body">
                    <form action="https://app.99inbound.com/api/e/hB5ewb94" target="_blank" method="POST">
                      <div class="p pb-3"><strong>Please, feel free to contact me </strong></div>
                      <div class="row mb-3">
                        <div class="col">
                          <div class="input-group"><span class="input-group-addon"><i class="fa fa-user-circle"></i></span>
                            <input class="form-control" type="text" name="name" placeholder="Name" required="required"/>
                          </div>
                        </div>
                      </div>
                      <div class="row mb-3">
                        <div class="col">
                          <div class="input-group"><span class="input-group-addon"><i class="fa fa-file-text"></i></span>
                            <input class="form-control" type="text" name="Subject" placeholder="Subject" required="required"/>
                          </div>
                        </div>
                      </div>
                      <div class="row mb-3">
                        <div class="col">
                          <div class="input-group"><span class="input-group-addon"><i class="fa fa-envelope"></i></span>
                            <input class="form-control" type="email" name="_replyto" placeholder="E-mail" required="required"/>
                          </div>
                        </div>
                      </div>
                      <div class="row mb-3">
                        <div class="col">
                          <div class="form-group">
                            <textarea class="form-control" name="message" placeholder="Your message" required="required"></textarea>
                          </div>
                        </div>
                      </div>
                      <div class="row">
                        <div class="col">
                          <button class="btn btn-primary" type="submit">Send</button>
                        </div>
                      </div>
                    </form>
                  </div>
                </div>
                <div class="col-md-6">
                  <div class="card-body">
                    <p class="mb-0"><strong>Dirección </strong></p>
                    <p class="pb-2">Merida, Mexico</p>
                    <p class="mb-0"><strong>Teléfono</strong></p>
                    <p class="pb-2">+521-55-8615-0878</p>
                    <p class="mb-0"><strong>Email</strong></p>
                    <p>jmhernandezr2@gmail.com</p>
                    <p class="mb-0"><strong>Availability</strong></p>
                    <p>Monday to Friday, 8:00 AM to 7:00 PM</p>
                  </div>  
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>

    

    <!-- FOOTER SECTION -->
    <footer class="footer">
      <div class="container text-center">
        <a class="btn btn-default btn-round btn-lg btn-icon smooth-scroll" href="#contact"  target="_blank" rel="tooltip" title="Contact Me"><i class="fa fa-phone"></i></a>
        <a class="btn btn-default btn-round btn-lg btn-icon" href="https://www.linkedin.com/in/jmhernandezr/" target="_blank" rel="tooltip" title="Follow me on LinkedIn"><i class="fa fa-linkedin"></i></a>
        <a class="btn btn-default btn-round btn-lg btn-icon" href="https://github.com/jmhrivera" target="_blank" rel="tooltip" title="Check out my repositories"><i class="fa fa-github"></i></a>
      <div class="h4 title text-center">Manuel Hernandez Rivera</div>
      <div class="text-center text-muted">
        <p>&copy; Creative CV. All rights reserved.<br>Design - <a class="credit" href="https://templateflip.com" target="_blank">TemplateFlip</a></p>
      </div>
    </footer>
    <script src="js/core/jquery.3.2.1.min.js"></script>
    <script src="js/core/popper.min.js"></script>
    <script src="js/core/bootstrap.min.js"></script>
    <script src="js/now-ui-kit.js?v=1.1.0"></script>
    <script src="js/aos.js"></script>
    <script src="scripts/main.js"></script>
  </body>
</html>